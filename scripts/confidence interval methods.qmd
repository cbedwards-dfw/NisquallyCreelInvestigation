---
title: "Exploring confint methods"
author: "Collin Edwards"
editor: visual
---

## Motivation

## Prep

```{r}
library("CreelEstimateR")
library("suncalc")
library("lubridate")
library("tidyverse")
library("patchwork")
library("gt")
library("here")
library("mgcv")
library("gratia")
library("framrsquared")
library("cli")
library("lme4")
```

### Functions

```{r}
confint_fromsample = function(p, #sample probability of marked (or other status of interest)
                              n, #total number of samples
                              level = 0.95 #confidence level for the confidence intervals. Default is 95% confidence intervals.
){
  ## input error checking
  p.clean = p[!is.na(p)]
  if(any(p.clean<0 | p.clean>1)){
    cli::cli_abort("`p` is a probability and must be between 0 and 1")
  }
  if(any(n %% 1 !=0 | n < 0)){
    cli::cli_abort("`n` must be a positive integer")
  }
  if(length(p) != length (n)){
    cli::cli_abort("`p` and `n` must have the same length.")
  }
  
  ## standard error for P
  se = sqrt((p)*(1-p)/(n-1))
  ## transform p and se into logit space
  ## The transformation of the se relies on the delta method:
  ## https://www.proteus.co.nz/news-tips-and-tricks/calculating-standard-errors-for-logistic-regressionlogit-link-using-the-delta-method
  
  beta = log(p/(1-p))
  se.beta = se/(p * (1-p))
  
  ## 95% confidence interval = mean +/- 1.96 * standard error
  multiplier = qnorm(1-(1-level)/2)
  conf = cbind(beta - multiplier * se.beta, beta + multiplier* se.beta)
  res = exp(conf)/(1+exp(conf))
  
  ## using likelihood profiling to handle cases when p = 0 or 1. 
  if(any( p.clean == 0 | p.clean == 1)){
    if(level != 0.95){
      cli::cli_alert_warning("Extreme values detected (p = 0 or p = 1). Current methods only calculate CIs correctly in these cases when level = 0.95")
    }else{
      ## NOT vectorized yet
      ind.do = which(p == 0 | p == 1)
      phats = seq(0.001, 0.999, by = 0.001)
      for(ind in ind.do){
        log.liks = log(dbinom(p[ind] * n[ind], prob = phats, size = n[ind]))
        conf = range(phats[log.liks - max(log.liks) > -2])
        res[ind,] = c(conf[1], conf[2])
      }
    }
  }
  ## transform confidence interval back from logit space into normal space
  
  colnames(res) = c("ci.low", "ci.high")
  return(res)
}

inv_logit = function(x) {exp(x)/(exp(x)+1)}
```


### Getting the data

```{r}
# test out the use of the newly packaged up fetch_dwg function, creating a vector of the names of the Nisqually fisheries we want to pull into R

# define fisheries we want data for
fisheries <- c("Nisqually salmon 2021", "Nisqually salmon 2022", "Nisqually salmon 2023")

# download the data from data.wa.gov
datapull <- set_names(fisheries) |> 
  map(~fetch_dwg(fishery_name = .x))

interviews <- datapull$`Nisqually salmon 2023`$interview |> 
  bind_rows(datapull$`Nisqually salmon 2022`$interview, datapull$`Nisqually salmon 2021`$interview)

# bind date and waterbody data to the creel interview-based catch records
catch <- datapull$`Nisqually salmon 2023`$catch |> 
  bind_rows(datapull$`Nisqually salmon 2022`$catch, datapull$`Nisqually salmon 2021`$catch) |>
  left_join(interviews |> 
              select(interview_id, event_date, water_body),
            by = "interview_id") |> 
  mutate(
    year = year(event_date),
    month = month(event_date)
  ) |> 
  filter(species == "Chinook") |> 
  mutate(week = framrsquared::statistical_week(event_date))
```


## Analysis

```{r}

dat = catch |> 
  filter(species %in% c("Chinook") 
         & (year == 2023) 
         & month %in% c(7,8,9,10)) |>
  filter(fin_mark %in% c("AD", "UM")) |> 
  group_by(interview_id, event_date, week) |> 
  summarize(UM = sum(fish_count[fin_mark == "UM"]),
            AD = sum(fish_count[fin_mark == "AD"]),
            tot = UM + AD) |> 
  ungroup()


dat.temp = dat |> 
  filter(event_date == event_date[1])
length(unique(dat.temp$interview_id))
out.glmer = glmer(cbind(AD, UM) ~ 1 + (1 | interview_id),
                  family = binomial,
                  data = dat.temp)
# summary(out.glmer)

conf = inv_logit(confint(out.glmer, "beta_", method = "Wald"))
conf.simple = confint_fromsample(p = sum(dat.temp$AD)/sum(dat.temp$tot),
                                 n = sum(dat.temp$tot))
tibble(est = inv_logit(fixef(out.glmer)),
       conf.low = conf[1],
       conf.high = conf[2],
       avg.raw = sum(dat.temp$AD)/sum(dat.temp$tot),
       avg.interview = mean(dat.temp$AD/dat.temp$tot),
       conf.simple.low = conf.simple[1],
       conf.simple.high = conf.simple[2]
) 

run_daily_fit <- function(dat.temp){
  ## fit model
  conf.simple <-  confint_fromsample(p = sum(dat.temp$AD)/sum(dat.temp$tot),
                                     n = sum(dat.temp$tot))
  if(any(!(dat.temp$UM/dat.temp$tot) %in% c(1, 0))){
    out.glmer <-  glmer(cbind(AD, UM) ~ 1 + (1 | interview_id),
                        family = binomial,
                        data = dat.temp)
    ## estimate confint
    conf <-  inv_logit(confint(out.glmer, "beta_", method = "Wald"))
    ## estimate confint old way
    ## summarize data
    tibble(est = inv_logit(fixef(out.glmer)),
           conf.low = conf[1],
           conf.high = conf[2],
           avg.raw = sum(dat.temp$AD)/sum(dat.temp$tot),
           avg.interview = mean(dat.temp$AD/dat.temp$tot),
           conf.simple.low = conf.simple[1],
           conf.simple.high = conf.simple[2]
    ) 
  }
  else{
    tibble(est = NA_real_,
           conf.low = NA_real_,
           conf.high = NA_real_,
           avg.raw = sum(dat.temp$AD)/sum(dat.temp$tot),
           avg.interview = mean(dat.temp$AD/dat.temp$tot),
           conf.simple.low = conf.simple[1],
           conf.simple.high = conf.simple[2]
    ) 
  }
}

dat.use <- dat |> 
  nest(.by = c(event_date)) |> 
  mutate(res = purrr::map(data, run_daily_fit)) |> 
  unnest(res)

ggplot(dat.use, aes(x = event_date, y = est))+
  geom_point()+
  geom_segment(aes(y = conf.low, yend = conf.high))
ggplot(dat.use, aes(x = event_date, y = est))+
  geom_point(aes(col = "mem"))+
  geom_segment(aes(y = conf.low, yend = conf.high, col = "mem"))+
  geom_point(aes(x = event_date+0.2, y = avg.raw, col = "simple"))+
  geom_segment(aes(x = event_date + 0.2,
                   y = conf.simple.low, yend = conf.simple.high,
                   col = "simple"))+
  labs(col = "Method",
       y = "Mark rate")+
  ggtitle("Comparing confint methods, 2023")
```


Trying a single model across all data

```{r}
events.use = names(table(dat$event_date)[table(dat$event_date)>1])

out =  glmer(cbind(AD, UM) ~ as.factor(event_date) + (1 | interview_id:event_date),
             family = binomial,
             data = dat |> 
               filter(event_date %in% events.use))

```

## Try on a weekly scale --> this should help fix the issues. 

```{r}
events.use = names(table(dat$event_date)[table(dat$event_date)>1])

out =  glmer(cbind(AD, UM) ~ 0 + as.factor(week) + (1 | interview_id),
             family = binomial,
             data = dat |> 
               filter(event_date %in% events.use),
             control = glmerControl(optimizer = "bobyqa"))

```

