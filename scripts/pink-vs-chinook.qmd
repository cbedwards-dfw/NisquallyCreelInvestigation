---
title: "data_pull"
author: "Collin Edwards, Evan Booher, Colt Holley, ... "
format: html
editor: visual
---

Quick note from Collin: Gavin has a tutorial on getting teh difference between two smooths and estimating uncertainty. Probably can adapt this to the quotient of two smooths. https://fromthebottomoftheheap.net/2017/12/14/difference-splines-ii/

```{r}
library("CreelEstimateR")
library("suncalc")
library("tidyverse")
library("patchwork")
library("gt")
library("here")
library("mgcv")
library("gratia")
library("framrsquared")
library("cli")
```

### From Evan - test pulling data and plot daily catch of Chinook and pink salmon from creel interviews

```{r read in data}
source(here("scripts/prepare_creel_data.R"))
catch = read_csv(here("cleaned_data/key_dataframes/creel_interview_catch.csv"),
                 col_types = c("cccccccncncDcnnn")) ##ugly column type specs, but read_csv was misidentifying sex and run types.
```

```{r basic plots}
# plot total daily encounters of Chinook and pink salmon for 2021
catch |> 
  filter(species %in% c("Pink", "Chinook") 
         & (year == 2021) 
         & month %in% c(7,8,9,10)) |> 
  group_by(year, event_date, species, fin_mark) |>
  summarise(daily_catch = sum(fish_count)) |> 
  ggplot(aes(event_date, daily_catch, color = species)) +
  ggtitle("2021") +
  geom_point()

# plot total daily encounters of Chinook and pink salmon for 2023
catch |> 
  filter(species %in% c("Pink", "Chinook") 
         & (year == 2023) 
         & month %in% c(7,8,9,10)) |> 
  group_by(year, event_date, species, fin_mark) |>
  summarise(daily_catch = sum(fish_count)) |> 
  ggplot(aes(event_date, daily_catch, color = species)) +
  ggtitle("2023") +
  geom_point()

# plot daily encounters of Chinook by fin_mark for 2023
catch |> 
  filter(species %in% c("Chinook") 
         & (year == 2023) 
         & month %in% c(7,8,9,10)) |> 
  group_by(year, event_date, species, fin_mark) |>
  summarise(daily_catch = sum(fish_count)) |> 
  ggplot(aes(event_date, daily_catch, color = fin_mark)) +
  ggtitle("2023") +
  scale_color_brewer(palette="Blues") +
  theme_bw() +
  geom_point(size = 3)


# next - add tribal net data and compare weekly or other time strata mark rates from net data and creel based interviews 


```

## Collin Modeling

Looking at smoothing splines fit to unmarked chinook and all pink salmon. Can we see a difference in phenology? Relative abundance?

### 2023

```{r}

spec_labeler = function(x){case_match(x, "Pink" ~ "Pink", "Chinook" ~ "Unmarked\nChinook")}

doy_2md=function(i){
  ymd=as.Date(i-1, origin="2019-01-01")
  return(format(ymd, "%b %d"))
}

dat.2023 = 
  catch |> 
  filter(species == "Pink" | species == "Chinook" & fin_mark == "UM") |> 
  count(year, event_date, species, name = "daily_catch") |> 
  mutate(doy = yday(event_date),
         species.fac = as.factor(species))

ggplot(dat.2023, aes(x = doy, y = daily_catch, col = species.fac))+
  geom_point(size = 3)+
  scale_x_continuous(labels = doy_2md)+
  theme_bw(base_size = 16)+
  scale_color_discrete(labels = spec_labeler)+
  xlab("date")+
  ggtitle("2023 Daily Encounters (Creel)")+
  labs(col = "Species")

## smoothing spline using a negative binomial distribution (good for count data)
out = gam(daily_catch ~ s(doy, by = species.fac),
          method = "REML", family = "nb",
          data = dat.2023)
## plotting the  smooths themselves:
draw(out)

## checking for issues the smooth flexibility
gam.check(out)

## looks good

dat.pred = expand_grid(doy = evenly(dat.2023$doy, by = 0.1),
                       species.fac = unique(dat.2023$species.fac)
)
dat.pred$daily_catch = predict(out, newdata = dat.pred, type = "response")

ggplot(dat.pred, aes(x = doy, y = daily_catch, col = species.fac))+
  geom_path(linewidth = 1)+
  geom_point(data = dat.2023, size = 3)+
  scale_x_continuous(labels = doy_2md)+
  scale_color_discrete(labels = spec_labeler)+
  theme_bw(base_size = 16)+
  xlab("date")+
  ggtitle("2023 Daily Encounters with smoothing splines")+
  labs(col = "Species")


pred.ratio.2023 = dat.pred |>  
  pivot_wider(names_from = species.fac, 
              values_from = daily_catch) |> 
  mutate(proportion.chinook = Chinook / (Chinook + Pink))
# dat.ratio.2023 = dat.2023 |> 
#   pivot_wider(names_from = species, 
#               values_from = daily_catch) |> 
#   mutate(proportion.chinook = Chinook / (Chinook + Pink))
ggplot(pred.ratio.2023, aes(x = doy, y = proportion.chinook))+
  geom_path()+
  theme_bw(base_size = 14)+
  scale_x_continuous(labels = doy_2md)+
  xlab("date")+
  ggtitle("2023 Proportion of Unmarked chinook")

## estimating uncertainty

# Something with this code, but I'm missing a couple of steps. I'm going to cheat
# and just use smooth_samples()
# 
# br = gam.mh(out, thin = 2, ns = 2000, rw.scale = .4)$bs
# ## matrix math time!
# Xp = predict(out, newdata = dat.pred, type = 'lpmatrix')
# ## normally we would need to drop columns not associated with the smooth, but ALL cols are associated with the smooth)
# Vb = vcov(out)
# beta_sim = rmvn(n = 20, coef(out), Vb)

sm_post = smooth_samples(out, "s(doy",
                         n = 20, seed = 42,
                         partial_match = TRUE)
draw(sm_post)

```

### 2021

```{r}
dat.2021 = 
  catch |> 
  filter(species == "Pink" | species == "Chinook" & fin_mark == "UM") |> 
  count(year, event_date, species, name = "daily_catch") |> 
  mutate(doy = yday(event_date),
         species.fac = as.factor(species))

## smoothing spline using a negative binomial distribution (good for count data)
out = gam(daily_catch ~ s(doy, by = species.fac),
          method = "REML", family = "nb",
          data = dat.2021)
## plotting the  smooths themselves:
draw(out)

## checking for issues the smooth flexibility
gam.check(out)

## looks good

dat.pred = expand_grid(doy = evenly(dat.2021$doy, by = 0.1),
                       species.fac = unique(dat.2021$species.fac)
)
dat.pred$daily_catch = predict(out, newdata = dat.pred, type = "response")

ggplot(dat.pred, aes(x = doy, y = daily_catch, col = species.fac))+
  geom_path()+
  geom_point(data = dat.2021)+
  scale_x_continuous(labels = doy_2md)+
  scale_color_discrete(labels = spec_labeler)+
  theme_bw(base_size = 14)+
  xlab("date")+
  ggtitle("2021 with smoothing splines")


pred.ratio.2021 = dat.pred |> 
  pivot_wider(names_from = species.fac, 
              values_from = daily_catch) |> 
  mutate(proportion.chinook = Chinook / (Chinook + Pink))
# dat.ratio.2021 = dat.2021 |> 
#   pivot_wider(names_from = species, 
#               values_from = daily_catch) |> 
#   mutate(proportion.chinook = Chinook / (Chinook + Pink))
ggplot(pred.ratio.2021, aes(x = doy, y = proportion.chinook))+
  geom_path()+
  theme_bw(base_size = 16)+
  scale_x_continuous(labels = doy_2md)+
  xlab("date")+
  ggtitle("2021 Proportion of \"Unmarked chinook\" that are unmarked chinook")

## estimating uncertainty

# Something with this code, but I'm missing a couple of steps. I'm going to cheat
# and just use smooth_samples()
# 
# br = gam.mh(out, thin = 2, ns = 2000, rw.scale = .4)$bs
# ## matrix math time!
# Xp = predict(out, newdata = dat.pred, type = 'lpmatrix')
# ## normally we would need to drop columns not associated with the smooth, but ALL cols are associated with the smooth)
# Vb = vcov(out)
# beta_sim = rmvn(n = 20, coef(out), Vb)

sm_post = smooth_samples(out, "s(doy",
                         n = 20, seed = 42,
                         partial_match = TRUE)
draw(sm_post)

```

## Colt - mark rate comparison

Importing tribal net data from cleaned_data/tidy_20-years-net-mark-rates.csv. Originally in Excel spreadsheet format (original_data/20 years Net Timing Nisqually Clipped Unclipped Chinook.xlsx), transformed to tidy format in scripts/tidy_net_mark_rates.R, and saved to cleaned_data folder.

```{r}
#import treaty net data
data_treaty_net <- readr::read_csv("./cleaned_data/tidy_20-years-net-mark-rates.csv")

#create date data frame
all_dates <- tibble(date = seq(as.Date("2004-01-01"), as.Date("2024-12-31"), by = "day"))

#add cr
all_dates <- all_dates |> 
  dplyr::mutate(
    management_week = framrsquared::management_week(date)
  )

#filter creel catch data to Chinook
data_creel_catch_Chinook <- catch |> 
  dplyr::filter(species == "Chinook",
                life_stage == "Adult") |> 
  #format to match tribal net dataframe
  dplyr::select(fin_mark, fate, fish_count, date = event_date, year)

#join management weeks with creel data
data_creel_catch_Chinook <- data_creel_catch_Chinook |> 
  dplyr::left_join(all_dates, by = "date")

#calc mark rates for creel data
data_creel_catch_Chinook <- data_creel_catch_Chinook |> 
  dplyr::filter(!fin_mark == "UNK") |> 
  dplyr::group_by(management_week, year) |> 
  dplyr::summarize(
    total_fish = sum(fish_count, na.rm = TRUE),
    ad_fish = sum(fish_count[fin_mark == "AD"], na.rm = TRUE),
    mark_rate = round( ad_fish / total_fish, 2)
  ) |> 
  dplyr::ungroup() |> 
  dplyr::arrange(year, management_week) |> 
  dplyr::mutate(data_source = "creel")

#calc mark rates for treaty data
data_treaty_net <- data_treaty_net |> 
  dplyr::group_by(year, week) |> 
  dplyr::summarise(
    total_fish = sum(catch, na.rm = T),
    ad_fish = sum(catch[fin_mark == "AD"], na.rm = T),
    mark_rate = round(ad_fish / total_fish, 2)
  ) |> 
  dplyr::mutate(data_source = "treaty") |> 
  dplyr::rename(management_week = week)

#combine
mark_rates <- rbind(data_treaty_net, data_creel_catch_Chinook)

```

```{r}
#plot mark rate comparison
# mark_rates |> 
#   dplyr::filter(year %in% c("2021", "2022", "2023")) |>
#   ggplot2::ggplot(aes(x = management_week, y = mark_rate, col = data_source)) +
#   geom_point(size = 2) +
#   scale_x_continuous(limits=c(min(mark_rates$management_week)-1, max(mark_rates$management_week)), n.breaks = 15) +
#   facet_wrap(.~ year, ncol = 1) +
#   labs(y = "Mark Rate (# AD / Total #)", x = "Management Week") +
#   guides(color = guide_legend(title = "Data Source")) +
#   theme_bw() +
#   theme(strip.text = element_text(size = 14),
#         axis.text = element_text(size = 10),
#         axis.title = element_text(size = 12))

```

```{r}
#original attempt at comparing mark rates with wrong input file
# data_tribal_net <- readr::read_csv(here("cleaned_data/tidy_net_mark_rates.csv"))
# 
# data_tribal_net |> 
#   filter(fate == "Kept") |> 
#   group_by(year, fin_mark) |> 
#   summarize(count = sum(fish_count, na.rm = T))
# 
# data_tribal_net |> 
#   filter(fate == "Released") |> 
#   group_by(year, fin_mark) |> 
#   summarize(count = sum(fish_count, na.rm = T))
# 
# #format tribal net data
# data_tribal_net <- data_tribal_net |> 
#   dplyr::mutate(
#     fate = dplyr::case_when(
#       fate == "released" ~ "Released",
#       fate == "kept" ~ "Kept",
#       TRUE ~ fate
#     ),
#     fin_mark = dplyr::case_when(
#       fin_mark == "ad" ~ "AD",
#       fin_mark == "um" ~ "UM",
#       TRUE ~ fin_mark
#     )
#   ) |> 
#   dplyr::rename(fish_count = catch)

#combined data frames

#take tribal stat_weeks and parse into dates by year
#join this with creel data to assign the same time periods to stat weeks
# stat_weeks <- data_tribal_net |> 
#   dplyr::select(year, stat_week, day_interval) |>
#   dplyr::mutate(
#     day_interval = dplyr::case_when(
#       day_interval == "10/33-11/06" ~ "10/31-11/06", #fixing error in dates
#       TRUE ~ day_interval 
#     ),
#     # Extract and parse start and end dates
#     start_date = lubridate::mdy(paste0(sub("-.*", "", day_interval), "/", year)),
#     end_date = lubridate::mdy(paste0(sub(".*-", "", day_interval), "/", year))
#   ) |> 
#   dplyr::select(-day_interval) |> 
#   dplyr::distinct()

## potential problem: IS THE CREEL DATA AGGREGATED DAILY?
# stat_weeks$event_date = purrr::map2(.x = stat_weeks$start_date, .y = stat_weeks$end_date, \(x, y) seq(x, y, by = "day"))
# 
# stat_weeks = stat_weeks |> 
#   unnest(event_date)

# #assign stat week to creel data
# data_creel_catch_Chinook <- data_creel_catch_Chinook |> 
#   dplyr::left_join(stat_weeks, by = c("year", "event_date"))|> 
#   dplyr::select(-start_date, -end_date, -event_date) |> 
#   dplyr::mutate(source = "creel")
# 
# #make id col
# data_tribal_net <- data_tribal_net |> 
#   dplyr::mutate(source = "treaty") |> 
#   dplyr::select(-day_interval)
# 
# #combine rec creel and tribal net data
# data_combined <- dplyr::bind_rows(data_creel_catch_Chinook, data_tribal_net)

#calculate mark rate
# mark_rates <- data_combined |> 
#   dplyr::filter(!fin_mark == "UNK") |> 
#   dplyr::group_by(source, year, stat_week) |> 
#   dplyr::summarize(
#     total_fish = sum(fish_count, na.rm = TRUE),
#     ad_fish = sum(fish_count[fin_mark == "AD"], na.rm = TRUE),
#     mark_rate = round( ad_fish / total_fish, 2)
#   ) |> 
#   dplyr::filter(!is.na(mark_rate)) |> 
#   dplyr::ungroup()

```

### More details

Collin has already written a function to calculate 95% confidence intervals from individual weekly samples of binomial data. This primarily uses the assymptotic method to calculate the confidence intervals in logit space and then transforms them back. When one of the two classes has 0 observations (e.g. entirely marked or unmarked fish in a week), the assymptotic method breaks down. In that case, the function instead uses likelihood profiling.

Code can also be found here: https://github.com/FRAMverse/snippets/blob/main/R/confint-fromsample.R.

```{r}
## calculate confidence intervals of binomial data from the proportion of successes and the total number of trials.
## Common use case: Our test fishing found a mark rate of 0.7 from 26 total fish. What's our confidence interval for that?
## Can be used for other classifications of fish as long as there are only 2 classes (e.g. "is legal size" vs "not legal size" or
## "is unmarked natural" vs "is not unmarked natural")
## 
## Primarily uses the assymptotic method to calculate confidence intervals in logit space and then transforms them back.
## Transformation of the standard error is based on the delta method.
## In cases in which p = 0 or p = 1 (e.g., all fish were marked or all fish were unmarked), assymptotic methods fail, and this
## uses likelihood profiling method instead. It was not obvious how to extend likelihood profiling to handle cases in which the level was 
## not 0.95, so in those cases the function gives a warning and returns the (meaningless) assymptotic estimate.
## 
## The function is vectorized, and can be used to calculate the CIs for an entire dataframe of fish data at once.

confint_fromsample = function(p, #sample probability of marked (or other status of interest)
                              n, #total number of samples
                              level = 0.95 #confidence level for the confidence intervals. Default is 95% confidence intervals.
){
  ## input error checking
  p.clean = p[!is.na(p)]
  if(any(p.clean<0 | p.clean>1)){
    cli::cli_abort("`p` is a probability and must be between 0 and 1")
  }
  if(any(n %% 1 !=0 | n < 0)){
    cli::cli_abort("`n` must be a positive integer")
  }
  if(length(p) != length (n)){
    cli::cli_abort("`p` and `n` must have the same length.")
  }
  
  ## standard error for P
  se = sqrt((p)*(1-p)/(n-1))
  ## transform p and se into logit space
  ## The transformation of the se relies on the delta method:
  ## https://www.proteus.co.nz/news-tips-and-tricks/calculating-standard-errors-for-logistic-regressionlogit-link-using-the-delta-method
  
  beta = log(p/(1-p))
  se.beta = se/(p * (1-p))
  
  ## 95% confidence interval = mean +/- 1.96 * standard error
  multiplier = qnorm(1-(1-level)/2)
  conf = cbind(beta - multiplier * se.beta, beta + multiplier* se.beta)
  res = exp(conf)/(1+exp(conf))
  
  ## using likelihood profiling to handle cases when p = 0 or 1. 
  if(any( p.clean == 0 | p.clean == 1)){
    if(level != 0.95){
      cli::cli_alert_warning("Extreme values detected (p = 0 or p = 1). Current methods only calculate CIs correctly in these cases when level = 0.95")
    }else{
      ## NOT vectorized yet
      ind.do = which(p == 0 | p == 1)
      phats = seq(0.001, 0.999, by = 0.001)
      for(ind in ind.do){
        log.liks = log(dbinom(p[ind] * n[ind], prob = phats, size = n[ind]))
        conf = range(phats[log.liks - max(log.liks) > -2])
        res[ind,] = c(conf[1], conf[2])
      }
    }
  }
  ## transform confidence interval back from logit space into normal space
  
  colnames(res) = c("ci.low", "ci.high")
  return(res)
}

# 
# 
# ### Examples -----------
# 
# ## works for individual cases
# confint_fromsample(p = .7, n = 26)
# 
# ## Can give vectors of p and n to provide multiple confidence intervals.
# confint_fromsample(p = c(0, 0.7, NA), n = c(26, 26, 0))

```

```{r}
mark_rates = cbind(mark_rates,
                   confint_fromsample(p = mark_rates$mark_rate, n = round(mark_rates$total_fish))
)

# #   mark_rates |> 
# #     arrange(year, stat_week) |>
# #   ## using code I developed here: https://github.com/FRAMverse/snippets/blob/main/R/adaptive%20confidence%20plotting.R
# #   ## in order to ensure that we have confidence envelopes = segments that correctly
# #   ## span the space that is sampled.
# #   mutate(ribbon.test = is.na(lag(mr, n = 1)) + is.na(lead(mr, n=1))) |>
# # #   ## make ribbon version of entries
# #   mutate(upper.ribbon = if_else(ribbon.test!=2, ci.high, NA),
# #          lower.ribbon = if_else(ribbon.test!=2, ci.low, NA)) |>
# #   mutate(upper.segment = if_else(ribbon.test==2, ci.high, NA),
# #          lower.segment = if_else(ribbon.test==2, ci.low, NA) ) |>
# #   select(-ribbon.test)
# 
# #plot
mark_rates = mark_rates |> 
  mutate(management_week_plot = management_week + .1 * (data_source == "creel"))

mark_rates |> 
  dplyr::filter(year %in% c("2021", "2022", "2023")) |>
  ggplot2::ggplot(aes(x = management_week_plot, y = mark_rate, col = data_source)) +
  geom_ribbon(aes(x = management_week_plot, ymin = ci.low, ymax = ci.high), alpha = 0.2)+
  ggplot2::geom_point(size = 2) +
  ggplot2::scale_x_continuous(limits=c(min(mark_rates$management_week)-1, max(mark_rates$management_week)), n.breaks = 15) +
  scale_color_discrete(labels = c("Recreational Creel", "Tribal Treaty Net")) +
  # scale_y_continuous(limits = c(0,1), expand = c(0.5,0.5)) +
  facet_wrap(.~ year, ncol = 1)+
  theme_bw() +
  labs(y = "Mark Rate (# AD / Total #)", x = "Management Week") +
  guides(color = guide_legend(title = "Data Source")) +
  theme_bw(base_size = 14)+
  theme(strip.text = element_text(size = 16),
        # axis.text = element_text(size = 10),
        # axis.title = element_text(size = 12)
  )+
  xlim(c(30,46))
ggsave(here("figures/weekly_MR.pdf"))

temp = mark_rates |>
  dplyr::filter(year %in% c("2021", "2022", "2023"),
                !is.nan(mark_rate)) |> 
  arrange(data_source, year, management_week) |>
  group_by(data_source, year) |> 
  complete(management_week = full_seq(management_week, period = 1)) |> 
  ## using code I developed here: https://github.com/FRAMverse/snippets/blob/main/R/adaptive%20confidence%20plotting.R
  ## in order to ensure that we have confidence envelopes = segments that correctly
  ## span the space that is sampled.
  mutate(ribbon.test = is.na(lag(mark_rate, n = 1)) + is.na(lead(mark_rate, n=1))) |>
  #   ## make ribbon version of entries
  mutate(upper.ribbon = if_else(ribbon.test!=2, ci.high, NA),
         lower.ribbon = if_else(ribbon.test!=2, ci.low, NA)) |>
  mutate(upper.segment = if_else(ribbon.test==2, ci.high, NA),
         lower.segment = if_else(ribbon.test==2, ci.low, NA) ) |>
  select(-ribbon.test) |> 
  mutate(ribbon.temp = is.na(upper.ribbon),
         ribbon.group = cumsum(ribbon.temp)) |> 
  ungroup() |> 
  select(-ribbon.temp)

temp |> 
  ggplot2::ggplot(aes(x = management_week_plot, y = mark_rate, col = data_source)) +
  geom_ribbon(aes(x = management_week_plot, ymin = lower.ribbon, ymax = upper.ribbon, col = data_source, group = interaction(data_source, ribbon.group)), 
              na.rm = FALSE, alpha = 0.2)+
  geom_segment(aes(x = management_week_plot, y = lower.segment, yend = upper.segment), linewidth = 0.8, alpha = 0.5)+
  ggplot2::geom_point(size = 2) +
  ggplot2::scale_x_continuous(limits=c(min(mark_rates$management_week)-1, max(mark_rates$management_week)), n.breaks = 15) +
  scale_color_discrete(labels = c("Recreational Creel", "Tribal Treaty Net")) +
  facet_grid(rows = vars(year))+
  xlab("Management week")+
  theme_bw() +
  labs(y = "Mark Rate (# AD / Total #)", x = "Management Week") +
  guides(color = guide_legend(title = "Data Source")) +
  theme(strip.text = element_text(size = 14),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12)
  )

gp.mr.2021 <-  temp |> 
  filter(year == 2021) |> 
  ggplot2::ggplot(aes(x = management_week_plot, y = mark_rate, col = data_source)) +
  geom_ribbon(aes(x = management_week_plot, ymin = lower.ribbon, ymax = upper.ribbon, 
                  col = data_source, group = interaction(data_source, ribbon.group)), 
              na.rm = FALSE, alpha = 0.2)+
  geom_segment(aes(x = management_week_plot, y = lower.segment, yend = upper.segment), linewidth = 0.8, alpha = 0.5)+
  ggplot2::geom_point(size = 2) +
  ggplot2::scale_x_continuous(n.breaks = 15,
                              limits = c(30, 46)) +
  scale_color_discrete(labels = c("Recreational Creel", "Tribal Treaty Net")) +
  # facet_grid(rows = vars(year))+
  xlab("Management week")+
  theme_bw(base_size = 14) +
  labs(y = "Mark Rate (# AD / Total #)", x = "Management Week") +
  guides(color = guide_legend(title = "Data Source")) +
  theme(strip.text = element_text(size = 14),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12)
  )


# ## making subplots for later stuff:
# gp.mr.2021 = temp |> 
#   filter(year == 2021) |> 
#   ggplot2::ggplot(aes(x = management_week_plot, y = mark_rate, col = data_source)) +
#   geom_ribbon(aes(x = management_week_plot, ymin = lower.ribbon, ymax = upper.ribbon, col = source, group = interaction(source, ribbon.group)), 
#               na.rm = FALSE, alpha = 0.2)+
#   geom_segment(aes(x = management_week_plot, y = lower.segment, yend = upper.segment), linewidth = 0.8, alpha = 0.5)+
#   ggplot2::geom_point(size = 2) +
#   ggplot2::scale_x_continuous(limits=c(min(mark_rates$management_week_plot)-1, max(mark_rates$management_week_plot)), n.breaks = 15) +
#   xlab("Management week")+
#   # facet_wrap(.~ year, ncol = 1)+
#   theme_bw(base_size = 14)

gp.mr.2023 <-  temp |> 
  filter(year == 2023) |> 
  ggplot2::ggplot(aes(x = management_week_plot, y = mark_rate, col = data_source)) +
  geom_ribbon(aes(x = management_week_plot, ymin = lower.ribbon, ymax = upper.ribbon, 
                  col = data_source, group = interaction(data_source, ribbon.group)), 
              na.rm = FALSE, alpha = 0.2)+
  geom_segment(aes(x = management_week_plot, y = lower.segment, yend = upper.segment), linewidth = 0.8, alpha = 0.5)+
  ggplot2::geom_point(size = 2) +
  ggplot2::scale_x_continuous(n.breaks = 15,
                              limits = c(30, 46)) +
  scale_color_discrete(labels = c("Recreational Creel", "Tribal Treaty Net")) +
  # facet_grid(rows = vars(year))+
  xlab("Management week")+
  theme_bw(base_size = 14) +
  labs(y = "Mark Rate (# AD / Total #)", x = "Management Week") +
  guides(color = guide_legend(title = "Data Source")) +
  theme(strip.text = element_text(size = 14),
        axis.text = element_text(size = 10),
        axis.title = element_text(size = 12)
  )
```

### Putting pink vs chinook on stat-week scale

```{r}
ratio.sum.2021 = pred.ratio.2021 |> 
  mutate(event_date = mdy("01/01/2021") + duration(round(doy-1), unit = "days"),
         management_week = framrsquared::management_week(event_date)) |> 
  group_by(management_week) |> 
  summarize(chinook = sum(Chinook), 
            pink = sum(Pink),
            proportion.chinook = chinook/(pink+chinook)
  )

gp.props.2021 = ggplot(ratio.sum.2021, aes(x = management_week, y = proportion.chinook))+
  geom_path()+
  geom_point()+
  xlab("Management week")+
  scale_x_continuous(limits=c(30,46), n.breaks = 15) +
  theme_bw(base_size = 14)

ratio.sum.2023 = pred.ratio.2023 |> 
  mutate(event_date = mdy("01/01/2023") + duration(round(doy-1), unit = "days"),
         management_week = framrsquared::management_week(event_date)) |> 
  group_by(management_week) |> 
  summarize(chinook = sum(Chinook), 
            pink = sum(Pink),
            proportion.chinook = chinook/(pink+chinook)
  )

gp.props.2023 = ggplot(ratio.sum.2023, aes(x = management_week, y = proportion.chinook))+
  geom_path()+
  geom_point()+
  xlab("Management week")+
  scale_x_continuous(limits=c(30,45), n.breaks = 15) +
  theme_bw(base_size = 14)

gp.mr.2021/gp.props.2021 & plot_annotation(title = "2021 comparison of mark rate and pink vs chinook")
ggsave(here("figures/weekly_MR_vs_pink_proportion 2021.pdf"), width = 12, height = 9)

gp.mr.2023/gp.props.2023 & plot_annotation(title = "2023 comparison of mark rate and pink vs chinook")
ggsave(here("figures/weekly_MR_vs_pink_proportion 2023.pdf"), width = 12, height = 9)


```

Data frame containing both the creel data and tribal net data.

::: {.scroll-container style="overflow-y: auto; height: 300px;"}
```{r}
data_combined |> 
  dplyr::relocate(source, year, stat_week, fin_mark, fate, fish_count) |> 
  gt::gt()
```
:::
